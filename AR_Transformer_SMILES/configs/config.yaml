# Configuration file for Inverse Polymer Design Pipeline

# Paths
paths:
  data_dir: "Data"
  polymer_file: "Data/Polymer/SMiPoly_polymers.gz"
  property_dir: "Data/Property"
  results_dir: "results"
  checkpoints_dir: "results/checkpoints"
  figures_dir: "results/figures"
  metrics_dir: "results/metrics"

# Data processing
data:
  random_seed: 42
  train_fraction: 1.0
  unlabeled_train_ratio: 0.95
  unlabeled_val_ratio: 0.05
  property_train_ratio: 0.8
  property_val_ratio: 0.1
  property_test_ratio: 0.1

# Tokenizer
tokenizer:
  special_tokens:
    pad: "[PAD]"
    mask: "[MASK]"
    bos: "[BOS]"
    eos: "[EOS]"
    unk: "[UNK]"
  max_length: 128

# Backbone model (GPT-2-small-like)
backbone:
  hidden_size: 384
  num_layers: 8
  num_heads: 8
  ffn_hidden_size: 1536
  dropout: 0.1
  max_position_embeddings: 256

# Small proxy model for hyperparameter tuning
proxy_backbone:
  hidden_size: 256
  num_layers: 4
  num_heads: 4
  ffn_hidden_size: 1024
  dropout: 0.1
  max_position_embeddings: 256

# Model size presets for scaling law experiments
model_sizes:
  small:
    # Architecture (depth-driven)
    hidden_size: 384
    num_layers: 6
    num_heads: 6
    ffn_hidden_size: 1536
    dropout: 0.1
    max_position_embeddings: 256
    # Training
    max_steps: 94460
    warmup_steps: 945
    batch_size: 128
    gradient_accumulation_steps: 4
    learning_rate: 2.598e-4
  medium:
    # Architecture (depth-driven)
    hidden_size: 768
    num_layers: 12
    num_heads: 12
    ffn_hidden_size: 3072
    dropout: 0.1
    max_position_embeddings: 256
    # Training
    max_steps: 70850
    warmup_steps: 708
    batch_size: 128
    gradient_accumulation_steps: 4
    learning_rate: 3.0e-4
  large:
    # Architecture (depth-driven)
    hidden_size: 1152
    num_layers: 18
    num_heads: 18
    ffn_hidden_size: 4608
    dropout: 0.1
    max_position_embeddings: 256
    # Training
    max_steps: 47230
    warmup_steps: 472
    batch_size: 128
    gradient_accumulation_steps: 8
    learning_rate: 3.518e-4
  xl:
    # Architecture (depth-driven)
    hidden_size: 1536
    num_layers: 24
    num_heads: 24
    ffn_hidden_size: 6144
    dropout: 0.1
    max_position_embeddings: 256
    # Training
    max_steps: 40480
    warmup_steps: 405
    batch_size: 64
    gradient_accumulation_steps: 16
    learning_rate: 3.824e-4

# Diffusion
diffusion:
  num_steps: 50
  beta_min: 0.05
  beta_max: 0.95

# Training backbone
training_backbone:
  batch_size: 256
  learning_rate: 3e-4
  weight_decay: 0.01
  warmup_steps: 1000
  max_steps: 300000
  gradient_clip_norm: 1.0
  eval_every: 1000
  val_max_samples: 100000  # Optional Step1 validation cap for faster periodic eval.
  save_every: 100000
  num_epochs: 50

# Training property heads
training_property:
  batch_size: 128
  learning_rate: 1e-3
  weight_decay: 0.01
  num_epochs: 500
  patience: 30
  freeze_backbone: true
  finetune_last_layers: 6
  default_timestep: 1

# Checkpointing
checkpointing:
  save_best_only: true         # default: only best checkpoint
  save_last: false             # set true to save *_last.pt
  save_periodic: false         # set true to save *_step_*.pt

# Optimization settings for 8xH100 default preset
optimization:
  use_amp: true                    # Mixed precision (BF16)
  compile_model: true              # torch.compile()
  compile_mode: default            # torch.compile() mode; avoid cudagraph issues
  optimizer_type: muon_adamw       # adamw | muon_adamw
  use_muon: true                   # Alias switch for optimizer_type=muon_adamw
  fp8_training: false              # Experimental FP8 path (H100/Hopper + torchao).
  fp8_backend: torchao             # Current FP8 backend integration.
  fp8_strict: false                # If true, fail fast when FP8 setup is unavailable.
  adamw_group_weight_decay: 0.0    # AdamW decay in Muon/AdamW mode (nanochat-style default 0).
  adam_embedding_lr: 3.0e-4        # AdamW LR for embeddings in Muon/AdamW mode.
  adam_unembedding_lr: 3.0e-4      # AdamW LR for unembedding/output head in Muon/AdamW mode.
  adam_scalar_lr: 3.0e-4           # AdamW LR for scalar/norm/bias parameters in Muon/AdamW mode.
  muon_matrix_lr: 2.0e-3           # Conservative Muon matrix LR for stable H100 starts.
  muon_weight_decay: 0.01          # Muon matrix weight decay.
  muon_momentum: 0.95              # Muon momentum coefficient.
  muon_ns_steps: 5                 # Newton-Schulz steps for Muon orthogonalization.
  muon_nesterov: true              # Use Nesterov-style momentum in Muon update.
  muon_dmodel_lr_scale: true       # Scale Adam-group LR by (d_model/768)^-0.5 in Muon mode.
  auto_batch_scaling: true         # Auto-scale global batch from model depth.
  auto_batch_ref_depth: 6          # Reference depth for auto batch scaling law.
  auto_batch_ref_global_batch: 1024  # Global batch at reference depth.
  auto_batch_exponent: 0.766       # Batch ~ depth^0.766 (derived from nanochat-style law).
  auto_batch_power_of_two: true    # Round auto batch target to nearest power-of-two.
  auto_batch_hardware_aware: true  # Detect runtime GPU memory/count and tune per-device micro-batch.
  auto_batch_ref_gpu_mem_gb: 80.0  # H100-class reference memory per GPU.
  auto_batch_ref_per_device_micro_batch: 128  # Reference per-device micro-batch at ref depth/memory.
  auto_batch_ref_seq_len_tokens: 256  # Reference sequence length for memory scaling.
  auto_batch_mem_depth_exponent: -0.35  # Micro-batch scales with depth^exponent.
  auto_batch_gpu_mem_safety_factor: 1.0  # <1.0 leaves extra headroom.
  auto_batch_min_per_device_micro_batch: 8
  auto_batch_max_per_device_micro_batch: 256
  auto_batch_micro_batch_multiple: 8
  compute_optimal_scaling: false    # Hard-written into model_sizes for 8xH100.
  compute_opt_target_param_data_ratio: 10.5  # Compute-optimal default from nanochat.
  compute_opt_seq_len_tokens: 256   # Token estimate per sample for token-budget calculations.
  compute_opt_ref_global_batch_tokens: 262144  # Reference B_ref tokens (1024 samples * 256 tokens).
  compute_opt_lr_batch_scaling: true  # Scale LR by sqrt(B/B_ref) using global token batch.
  cuda_allocator_guard: true        # Set PYTORCH_CUDA_ALLOC_CONF to reduce fragmentation OOMs.
  cuda_allocator_max_split_size_mb: 256
  cuda_allocator_gc_threshold: 0.8
  cuda_allocator_expandable_segments: true
  gradient_accumulation_steps: 4   # Effective batch = 256 * 4 = 1024
  num_workers: 4                   # DataLoader workers
  step1_num_workers: 0             # Step1 only: 0 = auto-select workers per rank from CPU budget
  step1_persistent_workers: true   # Step1 only: keep workers alive across epochs
  dynamic_padding: true            # Step1: pad to per-batch max length.
  length_bucket_sampler: false     # Step1: optional length-aware batching.
  bucket_size_multiplier: 50       # Step1: larger buckets improve length grouping.
  pin_memory: true                 # GPU memory pinning
  cudnn_benchmark: true            # cuDNN autotuning
  prefetch_factor: 2               # DataLoader prefetching
  cpu_oom_guard: true              # Auto-tighten loader settings when host RAM per rank is low.
  cpu_oom_guard_min_mem_gb_per_rank: 24.0
  cpu_oom_guard_max_workers: 8
  cpu_oom_guard_prefetch_factor: 1
  cpu_oom_guard_disable_pin_memory: true
  gpu_oom_guard_force_dynamic_padding: true
  gpu_oom_guard_low_mem_threshold_gb: 64.0
  cache_tokenization: false        # Avoid full in-memory cache on large datasets.
  cache_tokenization_max_samples: 500000  # Auto-disable cache above this many samples.

# Property head architecture
property_head:
  hidden_sizes: [256, 1024, 128]
  dropout: 0.1

# Hyperparameter tuning for property head (Step 3) using Optuna
hyperparameter_tuning:
  enabled: true                    # Enable via config or --tune flag
  n_trials: 100                     # Number of Optuna trials
  tuning_epochs: 35                 # Max epochs per trial
  tuning_patience: 10               # Early stopping patience per trial
  metric: "r2"                      # Optimization metric: maximize RÂ² on validation

  # Search space
  search_space:
    # Property head architecture: 3-5 layers, each layer size from [64, 128, 256, 512, 1024]
    num_layers: [3, 4, 5]
    neurons: [64, 128, 256, 512, 1024]

    # Training hyperparameters
    learning_rate: [4.0e-4, 6.0e-4, 8.0e-4, 1.0e-3]
    dropout: [0.1, 0.2, 0.3]
    finetune_last_layers_ratios: [0.0, 0.25, 0.5, 0.75, 1.0]
    batch_size: [8, 16, 32, 64, 128]  # 2^3 to 2^7

# Sampling
sampling:
  num_samples: 10000
  batch_size: 256
  temperature: 1.0
  use_constraints: true
  top_k: 0          # 0 disables top-k
  top_p: 1.0        # 1.0 disables top-p
  max_length: 128   # Max generation length for AR sampling

# Inverse design
inverse_design:
  num_candidates: 500
  epsilon: 30.0  # tolerance for property matching

# Polymer classes (SMARTS patterns)
polymer_classes:
  polyimide: "[#6](=O)-[#7]-[#6](=O)"
  polyester: "[#6](=O)-[#8]-[#6]"
  polyamide: "[#6](=O)-[#7]-[#6]"
  polyurethane: "[#8]-[#6](=O)-[#7]"
  polyether: "[#6]-[#8]-[#6]"

# Plotting
plotting:
  figure_size: [4.5, 4.5]
  font_size: 12
  dpi: 600
