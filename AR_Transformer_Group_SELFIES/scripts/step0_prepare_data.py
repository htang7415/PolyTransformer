#!/usr/bin/env python
"""Step 0: Prepare data and build Group SELFIES vocabulary and grammar."""

import os
import sys
import argparse
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))
repo_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(repo_root))

import random
import pandas as pd
import numpy as np

from src.utils.config import load_config, save_config
from src.utils.plotting import PlotUtils
from src.data.data_loader import PolymerDataLoader
from src.data.tokenizer import GroupSELFIESTokenizer
from src.utils.reproducibility import seed_everything, save_run_metadata
from shared.unlabeled_data import (
    require_preprocessed_unlabeled_splits,
)


def _export_step1_group_selfies_cache(
    df: pd.DataFrame,
    split_name: str,
    cache_path: Path,
    tokenizer: GroupSELFIESTokenizer,
    num_workers: int,
    pool_chunk_size: int,
    batch_size: int,
    canonicalize: bool
) -> dict:
    """Export chunked Step1 cache with precomputed Group SELFIES strings."""
    if batch_size <= 0:
        raise ValueError("group_selfies.step1_cache_batch_size must be > 0.")
    if pool_chunk_size <= 0:
        raise ValueError("group_selfies.step1_cache_chunk_size must be > 0.")

    cache_path.parent.mkdir(parents=True, exist_ok=True)
    if cache_path.exists():
        cache_path.unlink()

    total = len(df)
    kept_total = 0
    failed_total = 0

    print(
        f"   Exporting {split_name} Group SELFIES cache to {cache_path} "
        f"(rows={total:,}, batch_size={batch_size:,}, workers={num_workers})"
    )

    for start in range(0, total, batch_size):
        stop = min(start + batch_size, total)
        smiles_batch = df['p_smiles'].iloc[start:stop].astype(str).tolist()
        encoded_batch = tokenizer.parallel_encode_group_selfies(
            smiles_batch,
            num_workers=num_workers,
            chunk_size=pool_chunk_size,
            canonicalize=canonicalize,
            verbose=False
        )

        chunk_df = pd.DataFrame({
            'p_smiles': smiles_batch,
            'group_selfies': encoded_batch
        })
        failed = int(chunk_df['group_selfies'].isna().sum())
        failed_total += failed
        if failed > 0:
            chunk_df = chunk_df[chunk_df['group_selfies'].notna()].copy()

        chunk_df.to_csv(
            cache_path,
            mode='w' if start == 0 else 'a',
            header=(start == 0),
            index=False,
            compression='gzip'
        )
        kept_total += len(chunk_df)

        print(f"      {split_name}: {stop:,}/{total:,} rows processed")

    return {
        'split': split_name,
        'total': total,
        'kept': kept_total,
        'failed': failed_total,
        'canonicalize': canonicalize,
        'cache_path': str(cache_path),
    }


def main(args):
    """Main function."""
    # Load config
    config = load_config(args.config)

    # Create output directories
    results_dir = Path(config['paths']['results_dir'])
    step_dir = results_dir / 'step0_data_prep'
    metrics_dir = step_dir / 'metrics'
    figures_dir = step_dir / 'figures'
    results_dir.mkdir(parents=True, exist_ok=True)
    metrics_dir.mkdir(parents=True, exist_ok=True)
    figures_dir.mkdir(parents=True, exist_ok=True)

    # Reproducibility
    seed_info = seed_everything(config['data']['random_seed'])
    save_config(config, step_dir / 'config_used.yaml')
    save_run_metadata(step_dir, args.config, seed_info)

    # Initialize data loader
    data_loader = PolymerDataLoader(config)

    print("=" * 50)
    print("Step 0: Data Preparation (Group SELFIES)")
    print("=" * 50)

    # Load preprocessed shared unlabeled data
    print("\n1. Loading shared unlabeled train/val data...")
    train_shared_path, val_shared_path = require_preprocessed_unlabeled_splits(repo_root)
    train_df = pd.read_csv(train_shared_path)
    val_df = pd.read_csv(val_shared_path)
    print(f"Using shared train split: {train_shared_path}")
    print(f"Using shared val split: {val_shared_path}")

    # Build tokenizer vocabulary and grammar from training data only
    print("\n2. Building Group SELFIES grammar and vocabulary...")
    tokenizer = GroupSELFIESTokenizer(max_length=config['tokenizer']['max_length'])

    # Get group_selfies config
    gs_config = config.get('group_selfies', {})
    max_groups = gs_config.get('max_groups', 20000)
    grammar_sample_size = int(gs_config.get('grammar_sample_size', 0) or 0)
    roundtrip_test_size = int(gs_config.get('roundtrip_test_size', 0) or 0)
    allow_full_grammar_build = bool(gs_config.get('allow_full_grammar_build', False))
    allow_full_roundtrip_eval = bool(gs_config.get('allow_full_roundtrip_eval', False))
    auto_grammar_sample_cap = int(gs_config.get('auto_grammar_sample_cap', 1_000_000))
    auto_roundtrip_test_cap = int(gs_config.get('auto_roundtrip_test_cap', 200_000))

    # Get parallelization settings
    parallel_config = gs_config.get('parallel', {})
    num_workers = parallel_config.get('num_workers', 1)
    chunk_size = parallel_config.get('chunk_size', 1000)
    parallel_enabled = parallel_config.get('enabled', False)

    # Respect allocation limits to avoid OOM from worker over-subscription.
    available_workers = int(os.environ.get('SLURM_CPUS_PER_TASK') or (os.cpu_count() or 1))
    if num_workers == 0:
        num_workers = available_workers
        print(f"Auto-detected {num_workers} CPU workers from allocation")
    if num_workers > available_workers:
        print(f"Capping num_workers from {num_workers} to {available_workers} based on CPU allocation")
        num_workers = available_workers
    num_workers = max(1, num_workers)

    # Disable parallelization if not enabled
    if not parallel_enabled:
        num_workers = 1
        print("Parallelization disabled in config")

    train_count = len(train_df)

    # Safety guard: full-19M grammar build can exceed memory due to RDKit mol list materialization.
    if grammar_sample_size <= 0:
        if allow_full_grammar_build:
            effective_grammar_sample_size = train_count
            print("   Full grammar build explicitly enabled (allow_full_grammar_build=true).")
        else:
            cap = auto_grammar_sample_cap if auto_grammar_sample_cap > 0 else 1_000_000
            effective_grammar_sample_size = min(train_count, cap)
            if effective_grammar_sample_size < train_count:
                print(
                    "   Auto-capping grammar_sample_size to avoid OOM: "
                    f"{effective_grammar_sample_size:,}/{train_count:,}"
                )
    else:
        effective_grammar_sample_size = min(grammar_sample_size, train_count)
        if grammar_sample_size > train_count:
            print(
                f"   grammar_sample_size={grammar_sample_size:,} exceeds train size; "
                f"using {effective_grammar_sample_size:,}."
            )

    # Safety guard for expensive roundtrip verification.
    if roundtrip_test_size <= 0:
        if allow_full_roundtrip_eval:
            effective_roundtrip_test_size = 0  # 0 means full eval below
            print("   Full roundtrip evaluation explicitly enabled (allow_full_roundtrip_eval=true).")
        else:
            cap = auto_roundtrip_test_cap if auto_roundtrip_test_cap > 0 else 200_000
            effective_roundtrip_test_size = min(train_count, cap)
            if effective_roundtrip_test_size < train_count:
                print(
                    "   Auto-capping roundtrip_test_size for runtime/memory safety: "
                    f"{effective_roundtrip_test_size:,}/{train_count:,}"
                )
    else:
        effective_roundtrip_test_size = min(roundtrip_test_size, train_count)

    # Sample for grammar building if configured (much faster for large datasets)
    # IMPORTANT: We sample ONCE and split into grammar and test sets to ensure
    # the test molecules share the same structural distribution as the grammar molecules.
    total_sample_size = effective_grammar_sample_size + effective_roundtrip_test_size
    total_sample_size = min(train_count, total_sample_size)

    if total_sample_size < train_count:
        print(f"   Sampling {total_sample_size:,} molecules total (from {train_count:,})")
        sampled = train_df[['p_smiles']].sample(
            n=total_sample_size, random_state=config['data']['random_seed']
        ).reset_index(drop=True)
        total_sample = sampled['p_smiles'].tolist()
    else:
        total_sample = train_df['p_smiles'].tolist()

    grammar_smiles = total_sample[:effective_grammar_sample_size]
    train_roundtrip_sample = total_sample[effective_grammar_sample_size:]  # Held-out for testing

    print(f"   Grammar sample: {len(grammar_smiles):,}")
    if train_roundtrip_sample:
        print(f"   Held-out for roundtrip testing: {len(train_roundtrip_sample):,}")
    else:
        train_roundtrip_sample = None
        print("   No held-out roundtrip sample from grammar draw.")

    vocab, grammar = tokenizer.build_vocab_and_grammar(
        grammar_smiles,
        max_groups=max_groups,
        num_workers=num_workers,
        chunk_size=chunk_size,
        verbose=True
    )
    print(f"Vocabulary size: {tokenizer.vocab_size}")
    print(f"Placeholder token: {tokenizer.get_placeholder_token()}")
    print(f"Placeholder token ID: {tokenizer.get_placeholder_token_id()}")

    # Save tokenizer (pickle format for grammar)
    tokenizer_path = results_dir / 'tokenizer.pkl'
    tokenizer.save(tokenizer_path)
    print(f"Tokenizer saved to: {tokenizer_path}")

    # Optional Step1 acceleration cache: precompute Group SELFIES strings once in Step0.
    cache_enabled = bool(gs_config.get('step1_cache_enabled', True))
    cache_batch_size = int(gs_config.get('step1_cache_batch_size', 200000))
    cache_chunk_size = int(gs_config.get('step1_cache_chunk_size', chunk_size))
    cache_canonicalize = bool(gs_config.get('step1_cache_canonicalize', False))
    cache_train_file = str(gs_config.get('step1_cache_train_file', 'train_group_selfies_cache.csv.gz'))
    cache_val_file = str(gs_config.get('step1_cache_val_file', 'val_group_selfies_cache.csv.gz'))
    cache_stats = []
    if cache_enabled:
        print("\n3. Exporting precomputed Group SELFIES cache for Step1...")
        train_cache_path = results_dir / cache_train_file
        val_cache_path = results_dir / cache_val_file
        cache_stats.append(_export_step1_group_selfies_cache(
            train_df,
            'train',
            train_cache_path,
            tokenizer,
            num_workers=num_workers,
            pool_chunk_size=cache_chunk_size,
            batch_size=cache_batch_size,
            canonicalize=cache_canonicalize,
        ))
        cache_stats.append(_export_step1_group_selfies_cache(
            val_df,
            'val',
            val_cache_path,
            tokenizer,
            num_workers=num_workers,
            pool_chunk_size=cache_chunk_size,
            batch_size=cache_batch_size,
            canonicalize=cache_canonicalize,
        ))
        pd.DataFrame(cache_stats).to_csv(
            metrics_dir / 'step1_group_selfies_cache_stats.csv',
            index=False
        )
    else:
        print("\n3. Step1 Group SELFIES cache export disabled (group_selfies.step1_cache_enabled=false).")

    # Verify round-trip invertibility (PARALLELIZED with sampling)
    print("\n4. Verifying tokenization invertibility...")

    # 0 means full train/val roundtrip evaluation.
    roundtrip_test_size = effective_roundtrip_test_size

    # Use the held-out sample from grammar building (same distribution)
    # This is crucial: testing on molecules from a DIFFERENT random sample
    # causes 0.01% accuracy because they have different structural diversity.
    val_smiles_for_test = val_df['p_smiles'].tolist()

    if train_roundtrip_sample is not None and len(train_roundtrip_sample) > 0:
        # Use the held-out sample from the same random draw as grammar
        train_smiles_for_test = train_roundtrip_sample
        print(f"   Using {len(train_smiles_for_test):,} held-out molecules for train roundtrip (same distribution as grammar)")
    elif roundtrip_test_size > 0:
        # Fallback: sample directly from DataFrame to avoid materializing full 19M Python list.
        if roundtrip_test_size < len(train_df):
            print(f"   Sampling {roundtrip_test_size:,} molecules for train roundtrip (from {len(train_df):,})")
            train_smiles_for_test = train_df['p_smiles'].sample(
                n=roundtrip_test_size, random_state=config['data']['random_seed'] + 100
            ).tolist()
        else:
            train_smiles_for_test = train_df['p_smiles'].tolist()
    else:
        train_smiles_for_test = train_df['p_smiles'].tolist()

    # Sample validation set
    if roundtrip_test_size > 0:
        val_test_size = min(roundtrip_test_size // 10, len(val_df))  # 10% of train test size for val
        if val_test_size < len(val_smiles_for_test):
            print(f"   Sampling {val_test_size:,} molecules for val roundtrip (from {len(val_smiles_for_test):,})")
            val_smiles_for_test = val_df['p_smiles'].sample(
                n=val_test_size, random_state=config['data']['random_seed'] + 200
            ).tolist()

    # Verify train set (parallel now works - grammar recreated in each worker)
    train_valid, train_total, train_failures = tokenizer.parallel_verify_roundtrip(
        train_smiles_for_test,
        num_workers=num_workers,  # Parallel works: workers recreate grammar from group_smiles
        chunk_size=chunk_size,
        verbose=True
    )

    # Verify validation set
    val_valid, val_total, val_failures = tokenizer.parallel_verify_roundtrip(
        val_smiles_for_test,
        num_workers=num_workers,  # Parallel works: workers recreate grammar from group_smiles
        chunk_size=chunk_size,
        verbose=True
    )

    train_fail = train_total - train_valid
    val_fail = val_total - val_valid

    # Save roundtrip results
    roundtrip_df = pd.DataFrame({
        'split': ['train', 'val'],
        'total': [train_total, val_total],
        'valid': [train_valid, val_valid],
        'fail': [train_fail, val_fail],
        'pct': [100*train_valid/train_total, 100*val_valid/val_total]
    })
    roundtrip_df.to_csv(metrics_dir / 'tokenizer_roundtrip.csv', index=False)

    # Save failure diagnostics for quick inspection
    failure_rows = []
    for split, failures in (('train', train_failures), ('val', val_failures)):
        for failure in failures:
            if isinstance(failure, dict):
                row = {'split': split, **failure}
            else:
                row = {'split': split, 'smiles': str(failure), 'error': 'unknown'}
            failure_rows.append(row)
    if failure_rows:
        pd.DataFrame(failure_rows).to_csv(
            metrics_dir / 'tokenizer_roundtrip_failures.csv', index=False
        )

    # Save 10 example roundtrips for demonstration
    print("   Saving tokenization examples...")
    random.seed(config['data']['random_seed'])
    sample_smiles = random.sample(train_df['p_smiles'].tolist(), min(10, len(train_df)))

    examples = []
    for smiles in sample_smiles:
        tokens = tokenizer.tokenize(smiles)
        # Create token -> vocab ID hashmap
        token_ids = {tok: tokenizer.vocab.get(tok, tokenizer.unk_token_id) for tok in tokens}
        decoded = tokenizer.detokenize(tokens)
        examples.append({
            'original_smiles': smiles,
            'num_tokens': len(tokens),
            'group_selfies_tokens': str(tokens),
            'tokens_hashmap': str(token_ids),
            'decoded_smiles': decoded,
            'roundtrip_match': tokenizer.verify_roundtrip(smiles)
        })

    examples_df = pd.DataFrame(examples)
    examples_df.to_csv(metrics_dir / 'tokenizer_examples.csv', index=False)

    # Compute statistics
    print("\n5. Computing statistics...")
    train_stats = data_loader.get_statistics(train_df)
    val_stats = data_loader.get_statistics(val_df)

    # Save statistics
    stats_df = pd.DataFrame([
        {'split': 'train', **train_stats},
        {'split': 'val', **val_stats}
    ])
    stats_df.to_csv(metrics_dir / 'unlabeled_data_stats.csv', index=False)

    # Compute token lengths (PARALLELIZED)
    print("\n6. Computing token length distributions...")
    train_lengths = tokenizer.parallel_get_lengths(
        train_df['p_smiles'].tolist(),
        num_workers=num_workers,
        chunk_size=chunk_size,
        verbose=True
    )
    val_lengths = tokenizer.parallel_get_lengths(
        val_df['p_smiles'].tolist(),
        num_workers=num_workers,
        chunk_size=chunk_size,
        verbose=True
    )

    # Length statistics
    length_stats = pd.DataFrame({
        'split': ['train', 'val'],
        'mean': [np.mean(train_lengths), np.mean(val_lengths)],
        'std': [np.std(train_lengths), np.std(val_lengths)],
        'min': [np.min(train_lengths), np.min(val_lengths)],
        'max': [np.max(train_lengths), np.max(val_lengths)],
        'p95': [np.percentile(train_lengths, 95), np.percentile(val_lengths, 95)],
        'p99': [np.percentile(train_lengths, 99), np.percentile(val_lengths, 99)]
    })
    length_stats.to_csv(metrics_dir / 'length_stats.csv', index=False)

    # SA score statistics
    train_sa = train_df['sa_score'].dropna().values
    val_sa = val_df['sa_score'].dropna().values

    sa_stats = pd.DataFrame({
        'split': ['train', 'val'],
        'mean': [np.mean(train_sa), np.mean(val_sa)],
        'std': [np.std(train_sa), np.std(val_sa)],
        'min': [np.min(train_sa), np.min(val_sa)],
        'max': [np.max(train_sa), np.max(val_sa)]
    })
    sa_stats.to_csv(metrics_dir / 'sa_stats.csv', index=False)

    # Create plots
    print("\n7. Creating plots...")
    plotter = PlotUtils(
        figure_size=tuple(config['plotting']['figure_size']),
        font_size=config['plotting']['font_size'],
        dpi=config['plotting']['dpi']
    )

    # Length histogram
    plotter.histogram(
        data=[train_lengths, val_lengths],
        labels=['Train', 'Val'],
        xlabel='Token Length (Group SELFIES)',
        ylabel='Count',
        title='Group SELFIES Token Length Distribution',
        save_path=figures_dir / 'length_hist_train_val.png',
        bins=50,
        style='step'
    )

    # SA score histogram
    plotter.histogram(
        data=[train_sa, val_sa],
        labels=['Train', 'Val'],
        xlabel='SA Score',
        ylabel='Count',
        title='SA Score Distribution',
        save_path=figures_dir / 'sa_hist_train_val.png',
        bins=50,
        style='step'
    )

    print("\n8. Using shared split files directly...")
    print(f"  Train split: {train_shared_path}")
    print(f"  Val split: {val_shared_path}")

    # Print summary
    print("\n" + "=" * 50)
    print("Data preparation complete!")
    print("=" * 50)
    print(f"Train samples: {len(train_df)}")
    print(f"Val samples: {len(val_df)}")
    print(f"Vocabulary size: {tokenizer.vocab_size}")
    print(f"Placeholder token: {tokenizer.get_placeholder_token()}")
    print(f"Train roundtrip accuracy: {100*train_valid/train_total:.2f}%")
    print(f"Val roundtrip accuracy: {100*val_valid/val_total:.2f}%")
    print(f"Avg train token length: {np.mean(train_lengths):.2f}")
    print(f"Avg val token length: {np.mean(val_lengths):.2f}")
    print("=" * 50)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Prepare data and build Group SELFIES vocabulary')
    parser.add_argument('--config', type=str, default='configs/config.yaml',
                        help='Path to config file')
    args = parser.parse_args()
    main(args)
